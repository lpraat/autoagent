{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoAgentYOLO.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3NzOpjupwi5"
      },
      "source": [
        "# Prepare the codebase\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z8B2AHNwNaE"
      },
      "source": [
        "# Download repo\n",
        "!git clone https://github.com/lpraat/autoagent\n",
        "%cd autoagent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Du-yA-rvwgFz"
      },
      "source": [
        "# Set pythonpath\n",
        "%env PYTHONPATH=."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtt3q50Mrtu3"
      },
      "source": [
        "# Install required libraries\n",
        "!pip install -r requirements.txt\n",
        "# Also install gdown to ease google drive downloads\n",
        "!pip install gdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVsDO9A7r2Br"
      },
      "source": [
        "# Inference\n",
        "\n",
        "*NOTE*: By default, this repo uses the same model architecture used in the [Yolo-v5 Repository](https://github.com/ultralytics/yolov5.). You can find the definition in the /autoagent/models/vision/yolo/model_v5.py file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMUe3UAxr5gx"
      },
      "source": [
        "### Params\n",
        "We start by creating a yaml file, which is a config file containing all the parameters defining a model and its usage. An example config file can be found in /autoagent/models/vision/yolo/config/params.yaml.\n",
        "\n",
        "\n",
        "Below, you can find an hopefully-exhaustive description for each parameter. We now focus on those that are relevant for inference, and we will look at the remaining ones in the Training section below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4ncR6Y8ryNg"
      },
      "source": [
        "params = {\n",
        "  # Model Structure\n",
        "  # The model architecture, in this case v5 (small, medium, or large)\n",
        "  # In the example, we will load pre-trained weights for v5-large\n",
        "  'version': 'v5-large',\n",
        "  # Which activation to use, we select hswish as the original v5 weights\n",
        "  # work with that\n",
        "  'act': 'hswish',  # leaky, hswish, or add your preferred one\n",
        "  # Kind of convolution, leave it as-is as only conv type is defined\n",
        "  'conv': 'conv',\n",
        "  # Whether to initialize biases or not, irrelevant for inference.\n",
        "  'init_biases': True,\n",
        "\n",
        "  # Optimization\n",
        "  # Optimization parameters are irrelevant for inference\n",
        "  # Leave them as they are\n",
        "  # Reduction function used to compute final loss (either sum or mean)\n",
        "  'reduction': 'mean',\n",
        "  # Final loss is a weighted sum of localization, detection, and class loss\n",
        "  'loc_w': 0.05,\n",
        "  'det_w': 1,\n",
        "  'cls_w': 0.125,\n",
        "  # How much ciou loss influences detection loss (from 0 to 1)\n",
        "  'ciou_ratio': 1,\n",
        "  # Optimizer\n",
        "  'optim': 'sgd',\n",
        "  # Warmup epochs\n",
        "  'warmup': 3,\n",
        "  # Total number of epochs\n",
        "  'num_epochs': 300,\n",
        "  # Initial learning rate\n",
        "  # Lr warmups from 0 to 0.01 during the first warmup epochs\n",
        "  # then it is annealed using a cosine scheduler to final_lr\n",
        "  'init_lr': 0.01,\n",
        "  # Final learning rate\n",
        "  'final_lr': 0.002,\n",
        "  # Momentum warmups from 0.8 to 0.937 during the first warmup epochs\n",
        "  'warmup_momentum': 0.8,\n",
        "  'momentum': 0.937,\n",
        "  # Regularizer\n",
        "  'weight_decay': 0.0005,\n",
        "  # Balance coefficients for detection loss\n",
        "  'balance': [0.4, 1, 4],  # Large , medium, small output grids\n",
        "  # Exponential moving average parameters \n",
        "  # (https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage)\n",
        "  'ema_decay': 0.9999,\n",
        "  'ema_exp_d': 2000,\n",
        "  'ema_mode': 'exp',\n",
        "\n",
        "  # Eval\n",
        "  # Evaluation parameters are irrelevant for inference\n",
        "  # Leave them as they are\n",
        "  # Minimum acceptance confidence threshold\n",
        "  'confidence_thresh': 0.001,\n",
        "  # Non-maximum suppression threshold\n",
        "  'nms_thresh': 0.4,\n",
        "\n",
        "  # Img\n",
        "  # Augmentation parameters, irrelevant for inference\n",
        "  # Leave them as they are\n",
        "  # Sequence of augmentations to eventually apply to each image at training time\n",
        "  'augments': ['delta_bright', 'hflip'],\n",
        "  # Mosaic augmentation\n",
        "  # Probability to build a mosaic instead of a single image\n",
        "  'mosaic_prob': 1,\n",
        "  # Parameters influencing the affine transform used to build the mosaic\n",
        "  # See autoagents/models/yolo/data_wrapper\n",
        "  'mosaic_delta': 0.1,\n",
        "  'mosaic_scale': 0.5,\n",
        "  'mosaic_translate': 0.1,\n",
        "\n",
        "  # Target\n",
        "  # These parameters define how a prediction grid is transformed\n",
        "  # to retrieve the bbox predictions\n",
        "  # Anchor priors, as defined in v5\n",
        "  'anchor_priors': [\n",
        "    [[10, 13], [16, 30], [33, 23]],\n",
        "    [[30, 61], [62, 45], [59, 119]],\n",
        "    [[116, 90], [156, 198], [373, 326]]\n",
        "  ],\n",
        "  # Downsampled size at each prediction grid\n",
        "  # Step size at each \"grid division\". The v5 network structure downsamples\n",
        "  # the input image by a factor of 32, which is the output of the first grid (13x13 in case of a 416x416 input)\n",
        "  # the input image by a factor of 16, which is the output of the second grid (26x26) etc...\n",
        "  'steps': [32, 16, 8],\n",
        "  # Mode and mult_thresh are irrelevant for inference, leave them as they are\n",
        "  # they are used in the training phase to decide whether to map a target bounding box\n",
        "  # to a given anchor prior (multi_anchors introduced in v4). This repo supports two modes:\n",
        "  # 1) mult mode, the one used in yolov5, which assigns a bbox to an anchor prior\n",
        "  #    if bbox_width <= anchor_prior_width * mult_thresh and \n",
        "  #       bbox_height <= anchor_prior_height * mult_thresh\n",
        "  # 2) iou mode, which assigns a bbox to an anchor prior\n",
        "  #    if iou(bbox, anchor) >= iou_thresh\n",
        "  'mode': 'mult',\n",
        "  'mult_thresh': [4, 4, 4],\n",
        "  # mode: 'iou'\n",
        "  # iou_thresh: [0.2, 0.2, 0.2]\n",
        "  # The activation function used to obtain final bboxes from anchor_priors\n",
        "  # either sigmoid or exp\n",
        "  # 1) exp as in v4, bbox_wh = exp(pred_wh) * anchor_prior_wh\n",
        "  # 2) sigmoid as in v5, bbox_wh = (2*sigmoid(pred_wh))**2 * anchor_prior_wh\n",
        "  # Note that in case of sigmoid, at most we can output 4 times anchor_prior_wh\n",
        "  # and that's why the mult_thresh above is set to 4\n",
        "  'bbox_fn': 'sigmoid',\n",
        "\n",
        "  # COCO names\n",
        "  # We load pretrained weights from yolov5 repo, which are trained on COCO dataset\n",
        "  'cls_names': [\n",
        "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
        "    'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
        "    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
        "    'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
        "    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "    'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
        "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
        "    'hair drier', 'toothbrush'\n",
        "  ]\n",
        "}\n",
        "\n",
        "# Dump the parameters on a file, which we will provide for inference\n",
        "import yaml\n",
        "with open('./inference_params.yaml', mode='w') as f:\n",
        "  yaml.dump(params, f)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTb661DJzqSN"
      },
      "source": [
        "## Try Inference on an Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBhEsjpBzoPL"
      },
      "source": [
        "# Download pretrained Yolo-v5 COCO weights\n",
        "!gdown https://drive.google.com/uc?id=1wo7ftxMRxUhFNml7e2_Zl9rGqutQEJ6x\n",
        "!unzip yolov5weights.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofEVYYQ3zzoO"
      },
      "source": [
        "# View sample usage\n",
        "!python ./autoagent/models/vision/yolo/run.py --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSfFE6qNz1FY"
      },
      "source": [
        "# Run inference at 720p on a sample image\n",
        "!python ./autoagent/models/vision/yolo/run.py \\\n",
        "    --params ./inference_params.yaml \\\n",
        "    --ckpt ./yolov5weights/v5l.pt \\\n",
        "    --source ./autoagent/models/vision/yolo/sample_data/pic.png \\\n",
        "    --img_dim 1280 \\\n",
        "    --conf_thresh 0.25 \\\n",
        "    --nms_thresh 0.45 \\\n",
        "    --half_precision --cuda \\\n",
        "    --save_path ./result.jpg"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf4FWF-Qz8v9"
      },
      "source": [
        "# Visualize the results\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "result = cv2.imread('./result.jpg')\n",
        "cv2_imshow(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHxMDtfHz_Gx"
      },
      "source": [
        "# Training\n",
        "We will train the model on the [VOC](http://host.robots.ox.ac.uk/pascal/VOC/) dataset (2007trainval + 2012trainval for training, 2007 for validation), which is much smaller than the [COCO](https://cocodataset.org/#home) dataset and more suitable to show a training sample run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7Pv6nGC0Atf"
      },
      "source": [
        "params = {\n",
        "  # Model Structure\n",
        "  # The model architecture, in this case v5 (small, medium, or large)\n",
        "  'version': 'v5-large',\n",
        "  # Which activation to use\n",
        "  'act': 'hswish',  # leaky, hswish, or add your preferred one\n",
        "  # Kind of convolution, leave it as-is as only conv type is defined\n",
        "  'conv': 'conv',\n",
        "  # Whether to initialize biases or not.\n",
        "  'init_biases': True,\n",
        "\n",
        "  # Optimization\n",
        "  # Reduction function used to compute final loss (either sum or mean)\n",
        "  'reduction': 'mean',\n",
        "  # Final loss is a weighted sum of localization, detection, and class loss\n",
        "  'loc_w': 0.05,\n",
        "  'det_w': 1,\n",
        "  'cls_w': 0.125,\n",
        "  # How much ciou loss influences detection loss (from 0 to 1)\n",
        "  'ciou_ratio': 1,\n",
        "  # Optimizer\n",
        "  'optim': 'sgd',\n",
        "  # Warmup epochs\n",
        "  'warmup': 3,\n",
        "  # Total number of epochs\n",
        "  'num_epochs': 300,\n",
        "  # Initial learning rate\n",
        "  # Lr warmups from 0 to 0.01 during the first warmup epochs\n",
        "  # then it is annealed using a cosine scheduler to final_lr\n",
        "  'init_lr': 0.01,\n",
        "  # Final learning rate\n",
        "  'final_lr': 0.002,\n",
        "  # Momentum warmups from 0.8 to 0.937 during the first warmup epochs\n",
        "  'warmup_momentum': 0.8,\n",
        "  'momentum': 0.937,\n",
        "  # Regularizer\n",
        "  'weight_decay': 0.0005,\n",
        "  # Balance coefficients for detection loss\n",
        "  'balance': [0.4, 1, 4],  # Large , medium, small output grids\n",
        "  # Exponential moving average parameters \n",
        "  # (https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage)\n",
        "  'ema_decay': 0.9999,\n",
        "  'ema_exp_d': 2000,\n",
        "  'ema_mode': 'sigmoid',\n",
        "\n",
        "  # Eval\n",
        "  # Minimum acceptance confidence threshold\n",
        "  'confidence_thresh': 0.001,\n",
        "  # Non-maximum suppression threshold\n",
        "  'nms_thresh': 0.4,\n",
        "\n",
        "  # Img\n",
        "  # Sequence of augmentations to eventually apply to each image at training time\n",
        "  'augments': ['delta_bright', 'hflip'],\n",
        "  # Mosaic augmentation\n",
        "  # Probability to build a mosaic instead of a single image\n",
        "  'mosaic_prob': 1,\n",
        "  # Parameters influencing the affine transform used to build the mosaic\n",
        "  # See autoagents/models/yolo/data_wrapper\n",
        "  'mosaic_delta': 0.1,\n",
        "  'mosaic_scale': 0.5,\n",
        "  'mosaic_translate': 0.1,\n",
        "\n",
        "  # Target\n",
        "  # These parameters define how a prediction grid is transformed\n",
        "  # to retrieve the bbox predictions\n",
        "  # Anchor priors, as defined in v5\n",
        "  'anchor_priors': [\n",
        "    [[10, 13], [16, 30], [33, 23]],\n",
        "    [[30, 61], [62, 45], [59, 119]],\n",
        "    [[116, 90], [156, 198], [373, 326]]\n",
        "  ],\n",
        "  # Downsampled size at each prediction grid\n",
        "  # Step size at each \"grid division\". The v5 network structure downsamples\n",
        "  # the input image by a factor of 32, which is the output of the first grid (13x13 in case of a 416x416 input)\n",
        "  # the input image by a factor of 16, which is the output of the second grid (26x26) etc...\n",
        "  'steps': [32, 16, 8],\n",
        "  # Mode and mult_thresh are irrelevant for inference, leave them as they are\n",
        "  # they are used in the training phase to decide whether to map a target bounding box\n",
        "  # to a given anchor prior (multi_anchors introduced in v4). This repo supports two modes:\n",
        "  # 1) mult mode, the one used in yolov5, which assigns a bbox to an anchor prior\n",
        "  #    if bbox_width <= anchor_prior_width * mult_thresh and \n",
        "  #       bbox_height <= anchor_prior_height * mult_thresh\n",
        "  # 2) iou mode, which assigns a bbox to an anchor prior\n",
        "  #    if iou(bbox, anchor) >= iou_thresh\n",
        "  'mode': 'mult',\n",
        "  'mult_thresh': [4, 4, 4],\n",
        "  # mode: 'iou'\n",
        "  # iou_thresh: [0.2, 0.2, 0.2]\n",
        "  # The activation function used to obtain final bboxes from anchor_priors\n",
        "  # either sigmoid or exp\n",
        "  # 1) exp as in v4, bbox_wh = exp(pred_wh) * anchor_prior_wh\n",
        "  # 2) sigmoid as in v5, bbox_wh = (2*sigmoid(pred_wh))**2 * anchor_prior_wh\n",
        "  # Note that in case of sigmoid, at most we can output 4 times anchor_prior_wh\n",
        "  # and that's why the mult_thresh above is set to 4\n",
        "  'bbox_fn': 'sigmoid',\n",
        "\n",
        "  # VOC names\n",
        "  'cls_names': [\n",
        "    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat',\n",
        "    'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
        "    'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
        "  ]\n",
        "}\n",
        "\n",
        "# Dump the parameters on a file, which we will provide for inference\n",
        "import yaml\n",
        "with open('./training_params.yaml', mode='w') as f:\n",
        "  yaml.dump(params, f)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adF89c3v0DSS"
      },
      "source": [
        "## VOC training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDLOsWFR0E9Y"
      },
      "source": [
        "# Download the dataset\n",
        "!gdown https://drive.google.com/uc?id=1sTZHfpD6y37TBkWKFE4lz1EIaJGqCIty"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_ligN980GSC"
      },
      "source": [
        "# Place the data under /data folder\n",
        "!mkdir ./data\n",
        "!unrar x voc.rar ./data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH8Uy2e20HxE"
      },
      "source": [
        "# View sample usage\n",
        "!python ./autoagent/models/vision/yolo/train.py --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggjdxypQ0Kzv"
      },
      "source": [
        "# Start training\n",
        "# NOTE: The average precision is not computed (and set to 0) during warmup epochs (3)\n",
        "!python ./autoagent/models/vision/yolo/train.py \\\n",
        "  --params ./training_params.yaml \\\n",
        "  --data voc --batch_size 64 --aggregate 1 \\\n",
        "  --img_dim 416 --num_workers 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUgB1iEa0S5m"
      },
      "source": [
        "# Fine-tuning\n",
        "Let's fine tune a pre-trained v5-large model on VOC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U3Wt3YQ0TxP"
      },
      "source": [
        " params = {\n",
        "  # Model Structure\n",
        "  # The model architecture, in this case v5 (small, medium, or large)\n",
        "  'version': 'v5-large',\n",
        "  # Which activation to use\n",
        "  'act': 'hswish',  # leaky, hswish, or add your preferred one\n",
        "  # Kind of convolution, leave it as-is as only conv type is defined\n",
        "  'conv': 'conv',\n",
        "  # Whether to initialize biases or not.\n",
        "  'init_biases': True,\n",
        "\n",
        "  # Optimization\n",
        "  # Reduction function used to compute final loss (either sum or mean)\n",
        "  'reduction': 'mean',\n",
        "  # Final loss is a weighted sum of localization, detection, and class loss\n",
        "  'loc_w': 0.05,\n",
        "  'det_w': 1,\n",
        "  'cls_w': 0.125,\n",
        "  # How much ciou loss influences detection loss (from 0 to 1)\n",
        "  'ciou_ratio': 1,\n",
        "  # Optimizer\n",
        "  'optim': 'sgd',\n",
        "  # Warmup epochs\n",
        "  'warmup': 1,\n",
        "  # Total number of epochs\n",
        "  'num_epochs': 100,\n",
        "  # Initial learning rate\n",
        "  # Lr warmups from 0 to 0.01 during the first warmup epochs\n",
        "  # then it is annealed using a cosine scheduler to final_lr\n",
        "  'init_lr': 0.001,\n",
        "  # Final learning rate\n",
        "  'final_lr': 0.0002,\n",
        "  # Momentum warmups from 0.8 to 0.937 during the first warmup epochs\n",
        "  'warmup_momentum': 0.8,\n",
        "  'momentum': 0.9,\n",
        "  # Regularizer\n",
        "  'weight_decay': 0.0005,\n",
        "  # Balance coefficients for detection loss\n",
        "  'balance': [0.4, 1, 4],  # Large , medium, small output grids\n",
        "  # Exponential moving average parameters \n",
        "  # (https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage)\n",
        "  'ema_decay': 0.9999,\n",
        "  'ema_exp_d': 2000,\n",
        "  'ema_mode': 'exp',\n",
        "\n",
        "  # Eval\n",
        "  # Minimum acceptance confidence threshold\n",
        "  'confidence_thresh': 0.001,\n",
        "  # Non-maximum suppression threshold\n",
        "  'nms_thresh': 0.4,\n",
        "\n",
        "  # Img\n",
        "  # Sequence of augmentations to eventually apply to each image at training time\n",
        "  'augments': ['delta_bright', 'hflip'],\n",
        "  # Mosaic augmentation\n",
        "  # Probability to build a mosaic instead of a single image\n",
        "  'mosaic_prob': 1,\n",
        "  # Parameters influencing the affine transform used to build the mosaic\n",
        "  # See autoagents/models/yolo/data_wrapper\n",
        "  'mosaic_delta': 0.1,\n",
        "  'mosaic_scale': 0.5,\n",
        "  'mosaic_translate': 0.1,\n",
        "\n",
        "  # Target\n",
        "  # These parameters define how a prediction grid is transformed\n",
        "  # to retrieve the bbox predictions\n",
        "  # Anchor priors, as defined in v5\n",
        "  'anchor_priors': [\n",
        "    [[10, 13], [16, 30], [33, 23]],\n",
        "    [[30, 61], [62, 45], [59, 119]],\n",
        "    [[116, 90], [156, 198], [373, 326]]\n",
        "  ],\n",
        "  # Downsampled size at each prediction grid\n",
        "  # Step size at each \"grid division\". The v5 network structure downsamples\n",
        "  # the input image by a factor of 32, which is the output of the first grid (13x13 in case of a 416x416 input)\n",
        "  # the input image by a factor of 16, which is the output of the second grid (26x26) etc...\n",
        "  'steps': [32, 16, 8],\n",
        "  'mode': 'mult',\n",
        "  'mult_thresh': [4, 4, 4],\n",
        "  'bbox_fn': 'exp',\n",
        "\n",
        "  # VOC names\n",
        "  'cls_names': [\n",
        "    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat',\n",
        "    'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
        "    'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
        "  ]\n",
        "}\n",
        "\n",
        "# Dump the parameters on a file, which we will provide for inference\n",
        "import yaml\n",
        "with open('./fine_tune_params.yaml', mode='w') as f:\n",
        "  yaml.dump(params, f)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU6FHmeh0V0B"
      },
      "source": [
        "# Fine-tune training for 10 epochs\n",
        "!python ./autoagent/models/yolo/train.py \\\n",
        "  --params ./fine_tune_params.yaml \\\n",
        "  --data voc --batch_size 64 --aggregate 1 \\\n",
        "  --ckpt ./yolov5weights/v5l.pt --fine_tune \\\n",
        "  --img_dim 416 --num_workers 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KDpMpes5GtX"
      },
      "source": [
        "# View training statistics\n",
        "We can view the training statistics using tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puG_gy_45KHr"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./exp/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}